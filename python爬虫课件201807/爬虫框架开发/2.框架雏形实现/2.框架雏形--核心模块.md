## 框架雏形 -- 实现核心模块
##### 目标
1. 完成spider模块的封装
2. 完成调度器模块的封装
3. 完成下载器模块的封装
4. 完成管道模块的封装
5. 完成引擎模块的封装

### 1.spider模块的封装
##### 1.1 爬虫组件功能：
- 构建请求信息(初始的)，也就是生成请求对象(Request)
- 解析响应对象，返回数据对象(Item)或者新的请求对象(Request)

##### 1.2 实现方案：
1. 实现start_requests方法，返回请求对象
2. 实现parse方法，返回Item对象或者新的请求对象

##### 具体实现

```python
# scrapy_plus/core/spider.py
'''爬虫组件封装'''
from scrapy_plus.item import Item    # 导入Item对象
from scrapy_plus.http.request import Request    # 导入Request对象


class Spider(object):
    '''
    1. 构建请求信息(初始的)，也就是生成请求对象(Request)
    2. 解析响应对象，返回数据对象(Item)或者新的请求对象(Request)
    '''

    start_url = 'http://www.baidu.com'    # 默认初始请求地址   
                                      #这里以请求百度首页为例

    def start_requests(self):
        '''构建初始请求对象并返回'''
        return Request(self.start_url)

    def parse(self, response):
        '''解析请求
        并返回新的请求对象、或者数据对象
        '''
        return Item(response.body)   # 返回item对象
```

### 2. 调度器模块的封装
##### 2.1 调度器功能：
- 缓存请求对象(Request)，并为下载器提供请求对象，实现请求的调度：
- 对请求对象进行去重判断：实现去重方法`_filter_request`，该方法对内提供，因此设置为私有方法

##### 2.2 实现方案：
- 利用队列FIFO存储请求；
- 实现add_request方法添加请求，接收请求对象作为参数；
- 实现get_request方法对外提供从队列取出的请求对象

```python
# scrapy_plus/core/scheduler.py
'''调度器模块封住'''
# 利用six模块实现py2和py3兼容
from six.moves.queue import Queue


class Scheduler(object):
    '''
    1. 缓存请求对象(Request)，并为下载器提供请求对象，实现请求的调度
    2. 对请求对象进行去重判断
    '''
    def __init__(self):
        self.queue = Queue()

    def add_request(self, request):
        '''添加请求对象'''
        self.queue.put(request)

    def get_request(self):
        '''获取一个请求对象并返回'''
        request = self.queue.get()
        return request

    def _filter_request(self):
        '''请求去重'''
        # 暂时不实现
        pass
```

### 3 下载器模块的封装
##### 3.1 下载器功能：
- 根据请求对象(Request)，发起HTTP、HTTPS网络请求，拿到HTTP、HTTPS响应，构建响应对象(Response)并返回

##### 3.1 实现方案：
- 利用requests、urllib2等模块发请求，这里使用requests模块
- 实现get_response方法，接收request请求对象作为参数，发起请求，获取响应

```python
# scrapy_plus/core/downloader.py
'''下载器组件'''
import requests
from scrapy_plus.http.response import Response

class Downloader(object):
    '''根据请求对象(Request)，发起HTTP、HTTPS网络请求，拿到HTTP、HTTPS响应，构建响应对象(Response)并返回'''

    def get_response(self, request):
        '''发起请求获取响应的方法'''
        # 1. 根据请求对象，发起请求，获取响应
        #    判断请求方法：
        if request.method.upper() == 'GET':
            resp = requests.get(request.url, headers=request.headers,\
                          params=request.params)
        elif request.method.upper() == 'POST':
            resp = requests.post(request.url,headers=request.headers,\
                      params=request.params,data=request.data)
        else:
            # 如果方法不是get或者post，抛出一个异常
            raise Exception("不支持的请求方法")
        # 2. 构建响应对象,并返回
        return Response(resp.url, resp.status_code, resp.headers, resp.content)
```

### 4.管道模块的封装
##### 4.1 管道组件功能：
- 负责处理数据对象

##### 4.2 实现方案：
- 实现process_item方法，接收数据对象作为参数

```python
# scrapy_plus/core/pipeline.py
'''管道组件封装'''


class Pipeline(object):
    '''负责处理数据对象(Item)'''

    def process_item(self, item):
        '''处理item对象'''
        print("item: ", item)
```

### 5 引擎模块的封装
##### 5.1 引擎组件功能：
- 对外提供整个的程序的入口
- 依次调用其他组件对外提供的接口，实现整个框架的运作(驱动)

##### 5.2 实现方案：
- 利用init方法初始化其他组件对象，在内部使用
- 实现start方法，由外部调用，启动引擎
- 实现`_start_engine`方法，完成整个框架的运行逻辑
- 具体参考上一小节中雏形结构引擎的逻辑

```python
# scrapy_plus/core/engine.py
'''引擎组件'''
from scrapy_plus.http.request import Request    # 导入Request对象

from .scheduler import Scheduler
from .downloader import Downloader
from .pipeline import Pipeline
from .spider import Spider


class Engine(object):
    '''
    a. 对外提供整个的程序的入口
    b. 依次调用其他组件对外提供的接口，实现整个框架的运作(驱动)
    '''

    def __init__(self):
        self.spider = Spider()    # 接收爬虫对象
        self.scheduler = Scheduler()    # 初始化调度器对象
        self.downloader = Downloader()    # 初始化下载器对象
        self.pipeline = Pipeline()    # 初始化管道对象

    def start(self):
        '''启动整个引擎'''
        self._start_engine()

    def _start_engine(self):
        '''依次调用其他组件对外提供的接口，实现整个框架的运作(驱动)'''
        # 1. 爬虫模块发出初始请求
        start_request = self.spider.start_requests()

        # 2. 把初始请求添加给调度器
        self.scheduler.add_request(start_request)
        # 3. 从调度器获取请求对象，交给下载器发起请求，获取一个响应对象
        request = self.scheduler.get_request()
        # 4. 利用下载器发起请求
        response = self.downloader.get_response(request)

        # 5. 利用爬虫的解析响应的方法，处理响应，得到结果
        result = self.spider.parse(response)
        # 6. 判断结果对象
        # 6.1 如果是请求对象，那么就再交给调度器
        if isinstance(result, Request):
            self.scheduler.add_request(result)
        # 6.2 否则，就交给管道处理
        else:
            self.pipeline.process_item(result)
```

----

### 小结
1. 明确每个模块的功能以及需要实现的方法
2. 掌握引擎中实现逻辑的刘而成
3. 完成对爬虫框架的编写


